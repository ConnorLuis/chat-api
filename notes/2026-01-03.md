## 今日完成
- 设计并落地「可插拔 LLMEngine」结构：`generate()` / `stream()` 两个能力点（为后续接不同模型后端做准备）
- 新增 `MockEngine`：保持 Day2 的 mock 行为不回归（provider=mock 验收通过）
- 新增 `OllamaEngine`：通过 HTTP 调用 Windows 上的 Ollama（模型：`qwen2.5:7b`，Q4_K_M）
- 解决 WSL2 访问 Windows 服务的问题：确认 WSL 内 `127.0.0.1:11434` 不等于 Windows 的 `localhost:11434`，改用 Windows 网关 IP 访问
- 配置环境变量 `OLLAMA_BASE_URL` 并写入 `~/.bashrc`，保证新开 shell 自动生效
- 验收：`POST /chat` 在 `provider=ollama` 时可以返回真实模型回答（成功）

## 今日卡点
- WSL2 网络认知坑：Windows 上 Ollama `localhost:11434` 可用，但 WSL 内 `127.0.0.1:11434` 连接失败
  - 解决：在 WSL 中用 `WIN_IP=$(grep -m 1 nameserver /etc/resolv.conf | awk '{print $2}')` 取到 Windows 网关 IP，然后访问 `http://$WIN_IP:11434`
- 首次调用 `provider=ollama` 出现过一次 `Connection refused`
  - 可能原因：Ollama 服务刚重启/端口未就绪，或 FastAPI 进程没读到最新环境变量
  - 解决：确认 `echo $OLLAMA_BASE_URL` 生效，并在同一个 shell 启动 uvicorn；必要时重试一次

## 明日计划（不超过3条）
- 验收 `/chat/stream` 在 `provider=ollama` 下的流式输出（`curl -N ...`），并统一 stream 的输出格式
- 把 Ollama 配置进一步工程化：`base_url/model/timeout` 全部从环境变量读取（并在 README 写清楚）
- 加一份最小 README：如何启动、如何切 provider、常见排错（WSL/Windows 网络、11434、模型未 pull）

## 面试自测要点（要点式）
1) 为什么要做「Engine 抽象」？
   - 目的：把“业务接口层（FastAPI）”和“模型推理实现（Ollama/HF/云 API）”解耦，后端可替换、可扩展、可测试
2) 为什么要有 `provider=mock|ollama`？
   - 目的：开发阶段先用 mock 保证接口稳定与联调；接入真实模型后可一键切换，便于回归与故障降级
3) WSL2 为什么连不上 Windows 的 `127.0.0.1:11434`？
   - 因为 WSL2 有自己的网络命名空间，WSL 的 127.0.0.1 是“WSL 自己”，不是 Windows
   - 解决：用 Windows 网关 IP（`/etc/resolv.conf` 的 nameserver）访问
4) `/chat` 和 `/chat/stream` 的差别是什么？
   - `/chat` 一次性返回完整响应；`/chat/stream` 使用 `StreamingResponse` 分块返回（更像真实 ChatGPT 体验）
5) 为什么错误返回也要带 trace_id？
   - 线上排障需要“把客户端报错”和“服务端日志”对上号；trace_id 是最小可观测性闭环（尤其跨服务调用）

两个追问：
追问1：为什么要做 Engine 工厂，而不是在 routes 里 if/else？
    你答（15 秒）：
    “为了把依赖反转：routes 只依赖抽象 LLMEngine，不依赖具体实现。新增一个 provider 只要加一个 Engine 类和在工厂注册，不会污染路由逻辑，测试也更容易做 mock。”
追问2：stream() 为什么要异步？同步不行吗？
    你答（15 秒）：
    “流式输出本质是边接收边返回，底层是 I/O（HTTP 流）驱动，async 能在等待网络数据时不阻塞事件循环，让服务并发更好；同时 StreamingResponse 可以把 token 持续推给客户端，体验更像 ChatGPT。”
