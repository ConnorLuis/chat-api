## 今日完成
- WSL2 + Ubuntu 开发环境跑通
- conda 环境 chatapi（Python 3.10）可用
- FastAPI 服务启动成功
- Windows 可访问 WSL 服务：/health OK

## 今日卡点
- 使用命令行写py确实不变，更改为pycharm编辑，就流畅许多，且 python 解释器需要选择的miniforge下的envs下的bin内的python才行，之前一直是.virtualenvs下所以才有问题

## 明日计划（不超过3条）
- 增加 /chat（mock + schema）
- 增加 logging + trace_id 中间件
- 增加 /chat/stream（先 mock streaming）

## 面试自测要点（要点式）
1) token：
   - 定义：LLM处理文本不是按”字或词“，而是按**token*(子词/片段)**来切分。一个token可能是一个字、一个词的一部分、或一个短词
   - 作用：模型输入输出都以token计数
     - 成本：（很多模型按token计费）
     - 速度（生成越多token越慢）
     - 上下文长度（context window 也是token数量上限）
2) context window：
    - 定义：模型一次推理能”看到“的最大token数（包括：system+历史对话+你这次输入+它即将生成的输出）
    - 为什么重要：
      - 超过上限就会截断或报错
      - 历史越长，推理越慢、显存/内存越吃紧，成本越高
    - 工程常用做法：
      - 裁剪对话历史（只保留最近N轮）
      - 摘要历史（把长历史压成短摘要）
      - 用RAG把”知识“放在检索里，而不是塞进超长上下文
3) temperature/top_p：

    面试一句话：temperature 控制概率分布“变平/变尖”，top_p 控制“只在最可能的一部分里选”。

   - 采用控制输出随机性
   - temperature（温度）
     - 越低（接近0）：越确定、更”保守“、更像背标准答案（重复性高，稳定）
     - 越高：越发散、更有创意，但更可能胡说或跑题
     - 工程经验：问答/总结类：0-0.7；创意写作可更高
   - top_p(核采样)
     - 含义：只在”累计概率达到p的那一小撮候选token“里抽样
     - top_p越小：候选更小，输出越稳；越大：越发散
4) RAG为什么有效：

    RAG=Retrieval-Augmented Generation(检索增强生成)

   - 核心原因：LLM的参数记忆可能不全/不新/会幻觉；RAG先从你的知识库里检索证据，再让模型”基于证据回答“，等于把模型从凭记忆回答变成”看资料回答“。
   - 带来的收益：
     - 减少幻觉（因为有可引用证据）
     - 知识可更新（更新文档就更新能力，不用重新训练模型）
     - 成本更低（不用塞超长上下文，检索只拿最相关片段）
     - 可追溯（输出可附引用：doc/page/chunk）
   
    面试常见追问点：chunk 怎么切？召回/重排怎么做？怎么评测检索效果？
5) /health意义：
   - 定义：健康检查接口，用来判断服务是否”活着/可用“
   - 为什么重要：
     - 部署与监控：K8s/容器平台会用它做liveness/readiness probe（决定要不要重启/接流量）
     - 快速判断是“服务挂了”还是“某个业务接口逻辑挂了”
     - 自动化：CI/CD、网关、负载均衡都能依赖health做探测
   一句话总结：/health是线上稳定性的“生命体征”

