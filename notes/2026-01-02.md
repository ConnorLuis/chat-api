## 今日完成
- 新增 POST /chat（mock + Pydantic schema）
- 新增 trace_id 中间件（响应头 x-trace-id + latency log）
- 新增 POST /chat/stream（mock streaming）


## 今日验收
- /chat 返回 JSON + trace_id
- 响应头包含 x-trace-id
- /chat/stream 可流式输出

## 明日计划
- 抽象 LLM Engine 接口（engine_base）
- 接入 Ollama 作为真实推理后端（engine_ollama）

## 面试自测要点（要点式）
1) token追问：

    主回答

    Token 是大模型处理文本的最小单位（子词/片段），模型输入输出都按 token 计数。它直接影响三件事：上下文上限（context window）、推理速度（生成 token 越多越慢）、以及（如果用商业API）调用成本。工程上会控制 prompt 长度、裁剪历史、用 RAG 把知识放到检索里，避免无脑塞进上下文。

    追问 1：token 和“字/词”有什么区别？
    - 回答要点：token 不是固定等于字或词，是 tokenizer 分出来的子词。中文常接近“一字一 token”，英文常是“词的一部分”。所以字符数不等于 token 数，要用 tokenizer 统计才准。
    追问 2：你怎么在项目里控制 token？
    - 回答要点：
        1.记录输入/输出 token（日志里打出来）
        2.限制 max_tokens（防止输出爆长）
        3.历史对话只保留最近 N 轮或做摘要
        4.RAG 只拼 top-k 证据片段

对大段文档先切 chunk 再检索，不整篇塞进去
2) context window：

    主回答

    Context window 是模型一次推理能看到的最大 token 数，包含 system prompt、历史消息、用户输入、以及模型要生成的输出。超过窗口会截断或报错。窗口越大，推理越慢、显存/内存占用越高。工程上常用：**裁剪历史、摘要历史、检索增强（RAG）**来让上下文保持在可控范围。

    追问 1：为什么上下文越长就越慢/越吃显存？
    - 回答要点：注意力机制对序列长度很敏感；同时推理还有 KV cache，缓存越长占用越大。上下文变长会让计算量和缓存量上升，延迟和显存压力都会增加。
    追问 2：你的系统如何避免“对话越聊越爆长”？
    - 回答要点：
        1.简单版：只保留最近 N 轮（比如 6–10 轮）
        2.稍进阶：把更早的历史做一个摘要，替换掉原始对话
        3.最佳实践：涉及知识问答的内容用 RAG，从知识库检索证据，不依赖长历史记忆
3) temperature/top_p：

    主回答 

    Temperature 和 top_p 都是控制生成时“采样随机性”的参数。温度（temperature）越低分布越尖，输出越确定、稳定；越高越发散。top_p 是核采样：只在累计概率达到 p 的候选集合里抽样，p 越小越保守。工程上问答/提取类任务我会用较低温度（比如 0–0.7）和适中 top_p（0.8–0.95），保证稳定性。

    追问 1：temperature 和 top_p 你怎么选？有什么经验？
    - 回答要点：
        1.结构化输出/信息抽取：低温（0–0.3），top_p 也偏小，减少跑偏
        2.普通问答：温度 0.3–0.7
        3.创意写作：温度更高（>0.8）

    关键是“稳定性优先”，尤其线上系统要可控

    追问 2：模型输出不稳定/乱编，你从哪些参数先调？
    - 回答要点：先把 temperature 降低、适当降低 top_p、限制 max_tokens；同时要求“必须引用证据/只根据上下文回答”（RAG 场景）。如果仍乱编，回到检索质量、prompt 约束、以及是否需要拒答策略。
4) RAG为什么有效：

    主回答

    RAG（检索增强生成）有效是因为它把 LLM 从“凭参数记忆回答”变成“看资料回答”：先从知识库检索相关证据，再把证据拼到上下文让模型基于证据生成。这样能降低幻觉、知识可更新（换文档就行）、成本更低（不用长上下文）、还能可追溯（输出引用）。实际效果取决于检索链路质量：chunk、embedding、混合检索、重排、以及评测体系。

    追问 1：RAG 的效果主要由什么决定？你怎么优化？

    - 回答要点：
        1.切分策略（chunk_size/overlap/按标题层级）
        2.embedding 质量（领域适配、向量归一化）
        3.召回策略（向量 + BM25 混合）
        4.rerank 重排（把“看起来像”的筛成“真的相关”）
        5.评测与错误分析（先看 Recall@k，再分桶定位问题）
    
   追问 2：怎么评测 RAG？你会看哪些指标？

   - 回答要点：
        1.检索层：Recall@k / Hit@k（正确证据是否出现在 top-k）
        2.生成层：是否 grounded（答案是否能被引用支持），以及人工抽查典型 case
        3.工程上我会做一个 20–50 条小 evalset，每次改 chunk/检索策略都跑回归，避免“改崩了不知道”。
5) /health意义：

    主回答

    /health 是健康检查接口，用于判断服务是否可用。线上常分两类：liveness（进程活着没）和 readiness（能接流量没）。它服务于部署探针（K8s/容器）、负载均衡探测、监控告警和快速排障。我的项目里 /health 会至少返回状态，并可扩展检查依赖（比如向量库/模型服务是否可用）。
   追问 1：liveness 和 readiness 有什么区别？
   - 回答要点：
        1.liveness：服务是不是卡死/需要重启（探针失败会重启容器）
        2.readiness：服务是否准备好接流量（失败就不分配流量，但不一定重启）
    例如模型还在加载时 readiness 可以返回不就绪，但 liveness 仍然 ok。
   追问 2：你的 /health 会检查哪些依赖？
     - 回答要点：按项目阶段逐步加：
        1.v0：只返回 {"status":"ok"}
        2.v1：检查向量库连接/索引是否加载、模型服务（Ollama/推理服务）是否可访问
        3.v2：加“延迟阈值/磁盘空间/队列积压”等关键指标
    这样上线后能快速区分：是 API 挂了、还是依赖挂了。

